{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a397ee2-251f-4f8f-a0c0-f3334810e911",
   "metadata": {},
   "source": [
    "Q1. Running Ollama with Docker\n",
    "Let's run ollama with Docker. We will need to execute the same command as in the lectures:\n",
    "<code>\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ollama:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "</code>\n",
    "What's the version of ollama client?\n",
    "\n",
    "To find out, enter the container and execute ollama with the -v flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34a307-8bba-4e16-b477-0b3c8ac80139",
   "metadata": {},
   "source": [
    "### answer\n",
    "<code> docker exec -it ollama ollama -v </code> \\\n",
    "ollama version is 0.1.48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b70be9-fdf9-4988-8b66-bc31dcd6754e",
   "metadata": {},
   "source": [
    "Q2. We will donwload a smaller LLM - gemma:2b.\n",
    "Again let's enter the container and pull the model:\n",
    "<code> ollama pull gemma:2b</code>\n",
    "In docker, it saved the results into /root/.ollama\n",
    "We're interested in the metadata about this model. You can find it in models/manifests/registry.ollama.ai/library\n",
    "What's the content of the file related to gemma?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513e2ba-93ec-4a9e-86fe-3f8577537548",
   "metadata": {},
   "source": [
    "### answer\n",
    "<code> cd models/manifests/registry.ollama.ai/library/gemma </code> \\\n",
    "2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11adaf9f-8aa8-4ec3-b71b-cc9037a5e652",
   "metadata": {},
   "source": [
    "Q3. Running the LLM\n",
    "Test the following prompt: \"10 * 10\". What's the answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed18ed-e25a-4edc-8037-a1cb1a71768d",
   "metadata": {},
   "source": [
    "### answer\n",
    "<code> Sure, here is the answer:\n",
    "10 * 10<sup>end_of_turn</sup>\n",
    "This expression calculates 10 multiplied by 10 to the power of the end_of_turn digits. </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3ef65-fbbc-424a-b1aa-efa58a416b00",
   "metadata": {},
   "source": [
    "Q4. Donwloading the weights\n",
    "We don't want to pull the weights every time we run a docker container. Let's do it once and have them available every time we start a container.\n",
    "First, we will need to change how we run the container.\n",
    "Instead of mapping the /root/.ollama folder to a named volume, let's map it to a local directory:\n",
    "<code>\n",
    "mkdir ollama_files \\\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    -v ./ollama_files:/root/.ollama \\\n",
    "    -p 11434:11434 \\\n",
    "    --name ollama \\\n",
    "    ollama/ollama\n",
    "Now pull the model:\n",
    "docker exec -it ollama ollama pull gemma:2b \n",
    "</code>\n",
    "What's the size of the ollama_files/models folder?\n",
    "\n",
    "0.6G\n",
    "1.2G\n",
    "1.7G\n",
    "2.2G\n",
    "Hint: on linux, you can use du -h for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185d699-abda-44ff-ad92-1a6598a34db7",
   "metadata": {},
   "source": [
    "### answer\n",
    "<code> du -h ./root/.ollama/models \\\n",
    "8.0K    ./root/.ollama/models/manifests/registry.ollama.ai/library/gemma \\\n",
    "12K     ./root/.ollama/models/manifests/registry.ollama.ai/library \\\n",
    "16K     ./root/.ollama/models/manifests/registry.ollama.ai \\\n",
    "20K     ./root/.ollama/models/manifests \\\n",
    "1.6G    ./root/.ollama/models/blobs \\\n",
    "1.6G    ./root/.ollama/models </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b6f012-7a70-4eca-ad8f-5f8b510daf23",
   "metadata": {},
   "source": [
    "Q5. Adding the weights\n",
    "Let's now stop the container and add the weights to a new image\n",
    "For that, let's create a Dockerfile: \\\n",
    "<code>FROM ollama/ollama \\\n",
    "COPY ... </code> \\\n",
    "What do you put after COPY?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09e32b-6601-4c69-860b-0db45fa704ec",
   "metadata": {},
   "source": [
    "### answer\n",
    "<code> FROM ollama/ollama \\\n",
    "COPY ollama_files /root/.ollama/models </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb2e44-563a-4e1a-ba3c-dcce030af6d2",
   "metadata": {},
   "source": [
    "Q6. Serving it\n",
    "Let's build it: \\\n",
    "<code>docker build -t ollama-gemma2b </code> \\\n",
    "And run it:\\\n",
    "<code>docker run -it --rm -p 11434:11434 ollama-gemma2b</code>\\\n",
    "We can connect to it using the OpenAI client\n",
    "Let's test it with the following prompt: \\\n",
    "<code>prompt = \"What's the formula for energy?\"</code> \\\n",
    "Also, to make results reproducible, set the temperature parameter to 0: \\\n",
    "<code> response = client.chat.completions.create(\n",
    "    #...\n",
    "    temperature=0.0\n",
    ")</code> \\\n",
    "How many completion tokens did you get in response?\n",
    "304\n",
    "604\n",
    "904\n",
    "1204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df14f69-4276-432a-adb2-7259375a4080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
